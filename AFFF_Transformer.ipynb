{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTZrCpv2YFsf"
      },
      "outputs": [],
      "source": [
        "input_vars = ['MWCO', 'CA', 'Init_conc', 'Pressure', 'IS', 'mb_charge']\n",
        "\n",
        "train = pd.read_excel('AFFF_ML_data.xlsx', sheet_name='train')\n",
        "test = pd.read_excel('AFFF_ML_data.xlsx', sheet_name='test')\n",
        "\n",
        "df['SMILES'] = df['SMILES'].str.replace('[()=]', '', regex=True)\n",
        "df['group'] = df['ref_ID'].astype(str) + '_' + df['PFAS']\n",
        "df['Init_conc'] = df['Init_conc']/df['Mw']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyNu--CsvWHm"
      },
      "outputs": [],
      "source": [
        "from pytz import timezone\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, MolFromSmiles\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from transformers import AutoTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from hyperopt import STATUS_OK\n",
        "\n",
        "\n",
        "def positional_encoding(length, depth):\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)\n",
        "  angle_rads = positions * angle_rates\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  pos_encoding = pos_encoding[:, :depth]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "    self.pos_encoding = positional_encoding(length=d_model, depth=d_model)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = tf.expand_dims(x, -1)\n",
        "    x = self.dense(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[:length, :]\n",
        "    return x\n",
        "\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "    def call(self, x):\n",
        "        attn_output = self.mha(\n",
        "            query=x,\n",
        "            value=x,\n",
        "            key=x)\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, FF_num_layers, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential()\n",
        "    for _ in range(int(FF_num_layers)):\n",
        "        self.seq.add(tf.keras.layers.Dense(dff, activation='gelu'))\n",
        "        self.seq.add(tf.keras.layers.Dropout(dropout_rate))\n",
        "        dff = dff // 2\n",
        "    self.seq.add(tf.keras.layers.Dense(d_model))\n",
        "    self.linear = tf.keras.layers.Dense(d_model)\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    seq_output = self.seq(x)\n",
        "    x = self.add([x, seq_output])\n",
        "    x = self.linear(x)\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff, dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(d_model=d_model, )\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.pos_embedding(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "    return x\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(1, activation='linear')\n",
        "\n",
        "  @tf.function(reduce_retracing=True)\n",
        "  def call(self, inputs):\n",
        "    x = inputs\n",
        "    x = self.encoder(x)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    pred = self.final_layer(x)\n",
        "    return pred\n",
        "\n",
        "def scale_inputs(df, columns):\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df[columns])\n",
        "    return scaled_data, scaler\n",
        "\n",
        "def preprocess_smiles(smiles_list):\n",
        "    smiles_list = smiles_list.to_list()\n",
        "    d_model = 64\n",
        "\n",
        "    tokenizer = Tokenizer(char_level=True)\n",
        "    tokenizer.fit_on_texts(smiles_list)\n",
        "    sequences = tokenizer.texts_to_sequences(smiles_list)\n",
        "    max_length = max([len(seq) for seq in sequences])\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=56, padding='post')\n",
        "    return padded_sequences\n",
        "\n",
        "def hyp_opt(params):\n",
        "    tf.get_logger().setLevel('ERROR')\n",
        "    start_time = datetime.now(timezone('EST')).replace(microsecond=0)\n",
        "    num_heads = params['num_heads']\n",
        "    num_layers = params['num_layers']\n",
        "    learning_rate = params['learning_rate']\n",
        "    dff = params['dff']\n",
        "    dropout_rate = params['dropout_rate']\n",
        "    batch_size = params['batch_size']\n",
        "    epochs = params['epochs']\n",
        "    d_model = params['d_model']\n",
        "\n",
        "    mse_fold = []\n",
        "\n",
        "    X_train = train.copy()\n",
        "    y_train = train['Removal_rate']\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    encoded_molecular_data = preprocess_smiles(X_train['SMILES'])\n",
        "    encoded_molecular_data = pd.DataFrame(encoded_molecular_data)\n",
        "\n",
        "    X_train.reset_index(drop=True, inplace=True)\n",
        "    y_train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    train_scaled_inputs, scaler = scale_inputs(train, input_vars)\n",
        "    X_train_concate = pd.concat([pd.DataFrame(train_scaled_inputs), encoded_molecular_data], axis=1)\n",
        "\n",
        "    splitter = GroupKFold(n_splits=5)\n",
        "\n",
        "    best_params = None\n",
        "    best_RMSE = float('inf')\n",
        "    best_MAE = float('inf')\n",
        "\n",
        "    cv_rmse = []\n",
        "    cv_mae = []\n",
        "\n",
        "    for train_index, val_index in splitter.split(X_train, y_train, groups=train['group']):\n",
        "\n",
        "        fold_start = datetime.now(timezone('EST'))\n",
        "\n",
        "        X_train_fold, X_val_fold = tf.gather(X_train_concate, train_index), tf.gather(X_train_concate, val_index)\n",
        "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "        inputs = Input(shape=(d_model,))\n",
        "\n",
        "        transformer = Transformer(num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
        "                                  dff=dff, dropout_rate=dropout_rate)\n",
        "\n",
        "        outputs = transformer(inputs)\n",
        "\n",
        "        transformer.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "\n",
        "        history = transformer.fit(X_train_fold, y_train_fold,\n",
        "                                  validation_data=(X_val_fold, y_val_fold),\n",
        "                                  batch_size = batch_size,\n",
        "                                  epochs=epochs, verbose=0)\n",
        "\n",
        "        train_losses.append(history.history['loss'])\n",
        "        val_losses.append(history.history['val_loss'])\n",
        "\n",
        "        y_val_pred = transformer.predict(X_val_fold)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val_fold, y_val_pred))\n",
        "        mae = mean_absolute_error(y_val_fold, y_val_pred)\n",
        "\n",
        "        cv_rmse.append(rmse)\n",
        "        cv_mae.append(mae)\n",
        "        fold_end = datetime.now(timezone('EST'))\n",
        "\n",
        "    rmse_score = np.mean(cv_rmse)\n",
        "    mae_score = np.mean(cv_mae)\n",
        "\n",
        "    avg_train_loss = np.mean(train_losses, axis=0)\n",
        "    avg_val_loss = np.mean(val_losses, axis=0)\n",
        "\n",
        "    index_of_smallest = np.argmin(avg_val_loss)\n",
        "\n",
        "    return {'loss': rmse_score, 'status': STATUS_OK, 'params': best_params}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "d_model = 64\n",
        "num_heads_values = [11, 2, 4, 8, 16, 32]\n",
        "dff_values = [4, 8, 16, 32]\n",
        "num_layers_values = [1, 2, 3, 4, 5]\n",
        "batch_size_values = [16, 32, 64, 128, 256]\n",
        "FF_num_layers_values = [1, 2, 3, 4, 5]\n",
        "\n",
        "space = {'num_heads': hp.choice('num_heads', num_heads_values),\n",
        "         'num_layers': hp.choice('num_layers', num_layers_values),\n",
        "         'dff': hp.choice('dff', dff_values),\n",
        "         'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.1),\n",
        "         'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.1)),\n",
        "         'batch_size': hp.choice('batch_size', batch_size_values),\n",
        "         'epochs': hp.uniformint('epochs', 1, 1000),\n",
        "         'FF_num_layers': hp.choice('FF_num_layers', FF_num_layers_values),\n",
        "         'd_model': d_model}\n",
        "\n",
        "trials = Trials()\n",
        "best_params = fmin(fn=hyp_opt, space=space, algo=tpe.suggest, max_evals=30, trials=trials)\n",
        "\n",
        "best_params['num_heads'] = num_heads_values[best_params['num_heads']]\n",
        "best_params['num_layers'] = num_layers_values[best_params['num_layers']]\n",
        "best_params['dff'] = dff_values[best_params['dff']]\n",
        "best_params['batch_size'] = batch_size_values[best_params['batch_size']]\n",
        "best_params['FF_num_layers'] = FF_num_layers_values[best_params['FF_num_layers']]"
      ],
      "metadata": {
        "id": "RV56R1iiN9FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "encoded_molecular_data = pd.DataFrame(preprocess_smiles(train['SMILES']))\n",
        "d_model = 64\n",
        "\n",
        "train_scaled_inputs, scaler = scale_inputs(train, input_vars)\n",
        "X_train_concate = pd.concat([pd.DataFrame(train_scaled_inputs), encoded_molecular_data], axis=1)\n",
        "X_train_tf = tf.convert_to_tensor(X_train_concate)\n",
        "y_train = train['Removal_rate']\n",
        "\n",
        "inputs = Input(shape=(d_model,))\n",
        "\n",
        "transformer = Transformer(num_layers=best_params['num_layers'], d_model=d_model, num_heads=\n",
        "                          best_params['num_heads'], dff=best_params['dff'],\n",
        "                          dropout_rate=best_params['dropout_rate'])\n",
        "\n",
        "outputs = transformer(inputs)\n",
        "\n",
        "transformer.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss=customed_loss)\n",
        "\n",
        "history = transformer.fit(X_train_tf, y_train,\n",
        "                          batch_size = best_params['batch_size'],\n",
        "                          epochs=int(best_params['epochs']), verbose=1)\n",
        "\n",
        "X_test_scaled = scaler.fit_transform(test[input_vars])\n",
        "test_encoded_molecular_data = pd.DataFrame(preprocess_smiles(test['SMILES']))\n",
        "\n",
        "test.index = range(len(test))\n",
        "\n",
        "X_test_concate = pd.concat([pd.DataFrame(X_test_scaled), test_encoded_molecular_data], axis=1)\n",
        "X_test_tf = tf.convert_to_tensor(X_test_concate)\n",
        "y_test = test['Removal_rate']\n",
        "\n",
        "y_pred = transformer.predict(X_test_tf)\n",
        "test['y_pred'] = y_pred\n",
        "train_predictions = transformer.predict(X_train_tf)\n",
        "train['y_pred'] = train_predictions"
      ],
      "metadata": {
        "id": "2l83bPZXOQRh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}